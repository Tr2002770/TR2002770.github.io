[TOC]

# 机器学习

---

## 引言-认识ChatGpt

### 1. chatgpt真正做的事

![](https://picst.sunbangyan.cn/2023/10/14/yyv6il.jpg
)

输入一段文字后，通过chatgpt的function的计算，得出下一个概率最高的字，把得到的字与前面一段文字连接再次放入chatgpt计算，操作直到最高概率的字为【END】为止。

### 2. chatgpt的关键技术
- 预训练(Pro-train)

  使用尽可能多的训练数据，从中提取出尽可能多的共性特征，从而能让模型对特定任务的学习负担变轻。
- 监督式学习(Supervised Learning)

  给定一组数据，我们知道正确的输出结果应该是什么样子，并且知道在输入和输出之间有着一个特定的关系。
- 增强式学习(Reinforcement Learning)

  通过环境内部产生样本（特征和标签的匹配对）的监督学习。

---

## 机器学习基本概念

**机器学习 ≈ 机器寻找一个函式**

### 1.机器学习分类
- 回归(Regression)：函式的输出是一个数值
- 分类(Classification)：函式的输出是一个类别
![](https://picdl.sunbangyan.cn/2023/10/14/1102vhb.jpg)
### 2. 寻找函式的步骤
- 选出候选函式(Model)集合

  Deep Learning (CNN, Transformer ... ), Decision Tree, etc.
- 选出评价函式好坏的标准(Loss)

  Supervised Learning, Semi-supervised Learning, RL, etc.
- 找出最好的函式--> 最佳化 (Optimization)

  Gradient Descent , Genetic Algorithm, etc.
  
![寻找函式的步骤](https://picst.sunbangyan.cn/2023/10/14/12k4nsd.jpg)

### 3. 生成式AI学习的两种策略
生成式学习是用来生成有结构的复杂物件，这些复杂物件通常由一些小的元素构成：

- 文句：由 token 所构成
  - 中文的 token 就是【字】
  - 英文的 token 是 word piece。 unbreakable -> un break able
- 影像：由像素所组成
- 语音：由取样点所组成。16k 取样频率，每秒有16000个取样点

---

**策略一：各个击破——每次只生成一个元素。模型：Autoregressive(AR) model。**

**策略二：一次到位——一次性生成所有元素。模型：Non-autoregressive(NAR) model。**

---

各个击破 vs. 一次到位：

- 生成速度：通常一次到位策略速度更快。各个击破策略每个元素生成之前都要等前面的元素生成，时间开销大，没有办法平行化。一次到位策略只要有足够的平行运算能力，就可以把所有元素一次都生成出来。

- 生成品质：往往各个击破策略生成品质比较好，一次到位策略生成品质比较差。以生成文句举例：各个击破策略会生成第一个元素的概率分布，假设采样到【老】，再根据第一个元素来确定后续元素生成的结果，【老】后无疑是【师】概率比较大。一次到位策略会一次性生成第一个元素和第二个元素的概率分布，然后采样，则可能生成相邻词汇之间不匹配的情况。

![](https://picst.sunbangyan.cn/2023/10/14/12epz56.jpg)

**总结：AR 的生成品质更好，常用于生成文句。NAR 的生成速度更快，常用于生成影像。**

---
综合两种策略从而截长补短呢，有两种方法：

**方法一：先用各个击破策略决定大方向，然后用一次到位策略确定最后结果。常用于语音合成。**

![](https://picst.sunbangyan.cn/2023/10/14/12q0kjz.jpg)
**方法二：一次到位策略改为 N 次到位。相当于把各个击破策略融合进了一次到位策略。**
一次到位策略虽然能快速生成图像，但一次性生成的像素点之间可能存在不匹配情况，图像会非常模糊。可以把模糊的图像再通过一次到位策略变得更清楚一点，因为第一次生成图像的大方向已经决定了，之后的每次都会生成更清晰的图像。

![](https://picst.sunbangyan.cn/2023/10/14/12o6fx9.jpg)

---

## 深度学习

### 1.什么是深度学习？

深度学习是机器学习的一个分支，它基于人工神经网络模型，通过多层的非线性变换来学习数据特征。深度学习在图像识别、语音识别、自然语言处理等领域具有广泛的应用。

深度学习的发展历程
- 1980年代：反向传播算法的出现，使得神经网络训练成为可能。
- 2006年：Hinton教授等人提出了深度信念网络（DBN），开启了深度学习研究的新篇章。
- 2012年：AlexNet在ImageNet竞赛中取得了突破性的成绩，深度学习开始进入大众视野。

近年来：深度学习技术不断发展，涌现出了各种新的模型和应用场景。
常用的深度学习框架
- TensorFlow：Google推出的开源深度学习框架，支持多种硬件平台和编程语言。
- PyTorch：Facebook推出的开源深度学习框架，易于使用和调试。
- Keras：基于Python的高级神经网络API，可以运行在TensorFlow、Theano和CNTK后端上。
- Caffe：由伯克利实验室开发的深度学习框架，主要应用于计算机视觉领域。

### 2.神经网络和深度学习(Neural Networks and Deep Learning)

- 神经网络的编程基础(Basics of Neural Network programming)
  - 二分类(Binary Classification)
  这里有一个二分类问题的例子，假如你有一张图片作为输入，比如这只猫，如果识别这张图片为猫，则输出标签1作为结果；如果识别出不是猫，那么输出标签0作为结果。现在我们可以用字母y来表示输出的结果标签，如下图所示：
  <img src = "C:\Users\hw\Desktop\deeplearning\binary classification.png"/>
  计算机要保存一张图片，要保存三个独立矩阵，分别对应红，绿，蓝三个颜色通道,
  如果输入图片是64 x 64 像素（pixels）的，就有3个64 x 64的矩阵，分别对应rgb三种像素的亮度,要把这些像素亮度值，放进一个特征向量中，就要把这些像素值都提出来，放入一个特征向量x中：
  <img src = "C:\Users\hw\Desktop\deeplearning\251679b7c3d541719970621c06588d21(1).png"/>
  **在二分分类问题中，目标是训练出一个分类器，他以图片的特征向量x作为输入，预测输出的结果标签y 是1 or 0；**
  - 逻辑回归(Logistic Regression)
  对于二元分类问题来讲，给定一个输入特征向量，它可能对应一张图片，你想识别这张图片识别看它是否是一只猫或者不是一只猫的图片，你想要一个算法能够输出预测，你只能称之为$\hat y$，也就是你对实际值y的估计。更正式地来说，你想让$\hat y$表示$y$等于1的一种可能性或者是机会，前提条件是给定了输入特征$X$，你想让$\hat y$来告诉你这是一只猫的图片的机率有多大。$X$是一个$n_x$维的向量（相当于有$n_x$个特征的特征向量）。我们用$w$来表示逻辑回归的参数，这也是一个$n_x$维向量（因为实际上$w$是特征权重，维度与特征向量相同），参数里面还有$b$，这是一个实数（表示偏差）。所以给出输入$x$以及参数$w$和$b$之后，我们产生输出预测值$\hat y$，让$\hat y = w^{T}x + b$。
  <img src = "C:\Users\hw\Desktop\deeplearning\dfb5731c30b81eced917450d31e860a3.png"/>
  这时候我们经过线性回归得到的一个关于输入$x$的线性函数$\hat y$，但是这对于二元分类问题来讲不是一个非常好的算法，因为使用$\hat y$表示实际值$y$等于1的概率，那么$\hat y$应该在0到1之间，但实际情况是$\hat y = w^{T}x + b$可能要比1大的多，或者为一个负值，因此在逻辑回归中，我们的输出应该是$\hat y$等于由上面得到的线性函数式子作为自变量的sigmoid函数中，将线性函数转换为非线性函数。
  <img src = "C:\Users\hw\Desktop\deeplearning\R.jpg"/>
  $$\sigma(z) = \frac {1} {1+e^{-z}} ,z = w^{T}x + b$$
  - 逻辑回归的代价函数（Logistic Regression Cost Function）
    - 代价函数
    为了训练逻辑回归模型的参数参数$w$和参数$b$我们，需要一个代价函数，通过训练代价函数来得到参数$w$和参数$b$。先看一下逻辑回归的输出函数：<img src = "C:\Users\hw\Desktop\deeplearning\4c9a27b071ce9162dbbcdad3393061d2.png"/>
    为了让模型通过学习调整参数，你需要给予一个$m$样本的训练集，这会让你在训练集上找到参数$w$和参数$b$，来得到你的输出。对训练集的预测值，我们将它写成$\hat y$，我们更希望它会接近于训练集中的$y$值。
    - 损失函数
    损失函数又叫做误差函数，用来衡量算法的运行情况，Loss function:$L(\hat y,y)$。
    我们通过这个称为$L$的损失函数，来衡量预测输出值和实际值有多接近。
    我们在逻辑回归中用到的损失函数是：$L(\hat y,y) = -ylog(\hat y) - (1-y)log(1-\hat y)$
    注：$L(\hat y,y)$是一个凸函数(convex function)
    损失函数是在单个训练样本中定义的，它衡量的是算法在单个训练样本中表现如何，为了衡量算法在全部训练样本上的表现如何，我们需要定义一个算法的代价函数，算法的代价函数是对$m$个样本的损失函数求和然后除以$m$:$J(w,b) = \frac {1} {m}\Sigma_{i = 1}^{m} L(\hat y^{(i)},y^{(i)}) = \frac {1} {m}\Sigma_{i = 1}^{m} (-y^{(i)}log(\hat y^{(i)}) - (1-y^{(i)})log(1-\hat y^{(i)}))$
    损失函数只适用于像这样的单个训练样本，而代价函数是参数的总代价，所以在训练逻辑回归模型时候，我们需要找到合适的和，来让代价函数  的总代价降到最低。 
  - 梯度下降（Gradient Descent）
    - 梯度下降法可以做什么
    在测试集上，通过最小化代价函数（成本函数）$J(w,b)$来训练的参数w和参数b
    <img src = "C:\Users\hw\Desktop\deeplearning\cbd5ff8c461fcb5a699c4ec4789687b3.jpg"/>
    - 梯度下降法说明
    <img src = "C:\Users\hw\Desktop\deeplearning\a3c81d2c8629d674141def47dc02f312.jpg"/>
    横轴表示空间参数$w$和$b$(实际中，$w$可以是更高维度)，代价函数（成本函数）$J(w,b)$是在水平轴$w$和$b$上的曲面，因此曲面的高度就是$J(w,b)$在某一点的函数值。我们所做的就是找到使得代价函数（成本函数）$J(w,b)$函数值是最小值，对应的参数$w$和参数$b$。
      - 初始化参数$w$和参数$b$
      用如图那个小红点来初始化参数$w$和参数$b$(也可以采用随机初始化的方法)
      <img src = "C:\Users\hw\Desktop\deeplearning\0ad6c298d0ac25ca9b26546bb06d462c.jpg"/>
      - 朝最陡的下坡方向走一步，不断地迭代
      <img src = "C:\Users\hw\Desktop\deeplearning\bb909b874b2865e66eaf9a5d18cc00e5.jpg"/>
      <img src = "C:\Users\hw\Desktop\deeplearning\c5eda5608fd2f4d846559ed8e89ed33c.jpg"/>
      - 直到走到全局最优解或者接近全局最优解的地方
      通过以上的三个步骤我们可以找到全局最优解，也就是代价函数（成本函数）$J(w,b)$这个凸函数的最小值点。
    - 梯度下降法的细节化说明（仅有一个参数）
  - ......

---
 
## CNN(convolutional neural network)

---



